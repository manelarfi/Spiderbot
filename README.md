# SpiderBot

SpiderBot is a multithreaded web crawler that takes a target website URL, recursively crawls its pages, and stores the extracted data into a database. It is designed for efficient web scraping, indexing, or research purposes.

## Features

- Recursive website crawling
- Multithreaded execution for faster performance
- Data storage in a database (e.g., SQLite or MongoDB)
- Extracts URLs, titles, and metadata
- Customizable crawl depth and filtering

## Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/spiderbot.git
   cd crawler

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt

3. **Run The Project**
   ```bash
   python crawler_web.py
